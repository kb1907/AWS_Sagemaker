## **AWS Sagemaker Projects**
![](https://miro.medium.com/max/600/0*O3gm8pKaPqgKb-oM.png)
- In this folder, various-scale Machine Learning, Deep Learning, and NLP projects using AWS Sagemaker can be found.
- All the best

  
  ---------------------------------------
### AWS Sagemaker Projects
1. [AWS Sagemaker - Object Detection](https://github.com/kb1907/AWS_Sagemaker/blob/main/AWS_Sagemaker_Object_Detection/Sagemaker%20Object%20Detection%20-%20Learner%20Notebook.ipynb)

- In this project, I used AWS-Sagemaker for the object detection job. 

2. [AWS Sagemaker - House Prediction](https://github.com/kb1907/AWS_Sagemaker/blob/main/AWS_Sagemaker_House_Prediction/AWSSagemaker_House_Prediction.ipynb)

- In this project, I used classical house prediction data to show two different uses of AWS- Sagemaker (with Python SDK & with Boto3)

3. [BOTO3](https://github.com/kb1907/AWS_Sagemaker/blob/main/AWS_boto3/boto3_read_S3.ipynb)

- I made a gentle intro to the Boto3 library in this project.

![](https://cdn-images-1.medium.com/fit/t/700/400/1*DwPGGD3TmeBpxklRGsdfMA.png)
---------------------------------------
## Projects From [Practical Data Science Specialization](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/README.md)
------------------------------------------

4. [**AWS Data Wrangler** - Register and Visualize Dataset](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Analyze%20Datasets%20AutoML/Week1/C1_W1_Assignment_Learner.ipynb)

- I ingested and transformed the customer product reviews dataset in this project. 
- I used AWS data stack services, AWS Glue, and Amazon Athena to ingest and query the dataset. 
- Finally, I used AWS Data Wrangler to analyze the dataset and plot some visuals, extracting insights.

5. [Detect Data Bias with **Amazon SageMaker Clarify**](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Analyze%20Datasets%20AutoML/Week2/C1_W2_Assignment_Detect_data_bias_with_Amazon_SageMaker_Clarify.ipynb)


- Bias can be present in data before any model training occurs. 
- Inspecting the dataset for bias can help detect collection gaps, inform our feature engineering, and understand societal preferences the dataset may reflect. 
- In this project, I analyzed bias on the dataset, generated and analyzed bias reports, and prepared the dataset for the model training.

6. [Train a BERT Model with **Amazon SageMaker Autopilot**](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Analyze%20Datasets%20AutoML/Week3/C1_W3_Assignment.ipynb)

- I used Amazon Sagemaker Autopilot to train this project's BERT-based natural language processing (NLP) model. 
- The model analyzed customer feedback and classified the messages into positive (1), neutral (0), and negative (-1) sentiment.

7. [Train a Text Classifier using **Amazon SageMaker BlazingText** built-in algorithm](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Analyze%20Datasets%20AutoML/Week4/C1_W4_Assignment.ipynb)

- In this project, I used SageMaker BlazingText's built-in algorithm to predict the sentiment for each customer review. 
- BlazingText is a variant of FastText that is based on word2vec. 

8. [Feature transformation with Amazon SageMaker processing job and Feature Store](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Build%20Train%20and%20Deploy%20ML%20Pipelines%20using%20BERT/Week1/C2_W1_Assignment.ipynb)

- In this project, I started with the raw Women's Clothing Reviews dataset 
- I prepared it to train a BERT-based natural language processing (NLP) model. 


9. [Train a review classifier with BERT and Amazon SageMaker](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Build%20Train%20and%20Deploy%20ML%20Pipelines%20using%20BERT/Week2/C2_W2_Assignment.ipynb)

- In this project, I trained a text classifier using a variant of BERT called RoBERTa - a Robustly Optimized BERT Pretraining Approach - within a PyTorch model run as a SageMaker Training Job.

10. [Optimize models using Automatic Model Tuning](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Optimize%20ML%20Models%20and%20Deploy%20Human-in-the-Loop%20Pipelines/Week1/C3_W1_Assignment.ipynb)

-  In this project, I applied a random algorithm of Automated Hyperparameter Tuning to train a BERT-based natural language processing (NLP) classifier. 

11. [A/B testing, traffic shifting, and autoscaling](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Optimize%20ML%20Models%20and%20Deploy%20Human-in-the-Loop%20Pipelines/Week2/C3_W2_Assignment.ipynb)

- In this project, I made an endpoint with multiple variants, splitting the traffic between them. 
- After testing and reviewing the endpoint performance metrics, I shifted the traffic to one variant and configure it to autoscale.

12. [Data labeling and human-in-the-loop pipelines with Amazon Augmented AI (A2I)](https://github.com/kb1907/Practical-Data-Science-Specialization/blob/main/Optimize%20ML%20Models%20and%20Deploy%20Human-in-the-Loop%20Pipelines/Week3/C3_W3_Assignment.ipynb)

- In this project, I made my human workforce, a human task UI and then defined the human review workflow to perform data labeling. 
- I made the original predictions of the labels with the custom ML model and then made a human loop if the probability scores are lower than the preset threshold. 
- After the completion of the human loop tasks, I reviewed the results and prepare data for re-training.



-------------------------------------------------

[AWS Sagemaker examples - Forked from aws/amazon-sagemaker-examples](https://github.com/kb1907/amazon-sagemaker-examples) 
